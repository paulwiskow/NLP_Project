{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad4ecaf",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2 on Movie Script Data\n",
    "\n",
    "This notebook fine-tunes a GPT-2 model on processed movie script data to generate dialogue and scene descriptions in the style of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f48e7",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf877c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d5387",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare the Processed Script Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db699a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed script text file directly\n",
    "with open('processed_scripts/star_wars_processed.txt', 'r', encoding='utf-8') as f:\n",
    "    training_text = f.read()\n",
    "\n",
    "print(f\"Loaded script with {len(training_text)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(training_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e393df",
   "metadata": {},
   "source": [
    "## 3. Load GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e940682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GPT-2 model and tokenizer\n",
    "# Options: \"gpt2\" (124M), \"gpt2-medium\" (355M), \"gpt2-large\" (774M), \"gpt2-xl\" (1.5B)\n",
    "model_name = \"gpt2-medium\"  # Using larger model for better quality\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f149e7",
   "metadata": {},
   "source": [
    "## 4. Tokenize and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f565141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks for training\n",
    "# GPT-2 has a max context length of 1024 tokens\n",
    "max_length = 512  # Use smaller chunks for efficiency\n",
    "\n",
    "# Tokenize the entire text\n",
    "tokenized = tokenizer(training_text, return_tensors='pt', truncation=False)\n",
    "input_ids = tokenized['input_ids'][0]\n",
    "\n",
    "print(f\"Total tokens: {len(input_ids)}\")\n",
    "\n",
    "# Split into chunks\n",
    "def create_chunks(input_ids, chunk_size):\n",
    "    \"\"\"Split input_ids into chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_ids) - chunk_size, chunk_size // 2):  # 50% overlap\n",
    "        chunk = input_ids[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk.tolist())\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks(input_ids, max_length)\n",
    "print(f\"Created {len(chunks)} training chunks\")\n",
    "\n",
    "# Create dataset\n",
    "dataset_dict = {'input_ids': chunks}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split into train and validation\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a041b",
   "metadata": {},
   "source": [
    "## 5. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2070732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments - optimized for gpt2-medium with limited memory\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-movie-script\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,  # More epochs for better learning\n",
    "    per_device_train_batch_size=2,  # Smaller batch for gpt2-medium memory requirements\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,  # More frequent evaluation\n",
    "    save_steps=100,  # Must match eval_steps when using load_best_model_at_end\n",
    "    warmup_steps=100,  # Gradual warmup\n",
    "    learning_rate=5e-5,  # Slightly higher for more aggressive learning\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=25,  # More frequent logging\n",
    "    save_total_limit=2,  # Keep fewer checkpoints to save space\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,  # Load best model after training\n",
    "    metric_for_best_model=\"loss\",\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "\n",
    "print(f\"  Total training steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c904b",
   "metadata": {},
   "source": [
    "## 6. Create Data Collator and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPT-2 uses causal language modeling, not masked LM\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized and ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93027db0",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"Model saved to: ./gpt2-movie-script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45b721",
   "metadata": {},
   "source": [
    "## 8. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and tokenizer\n",
    "model.save_pretrained(\"./gpt2-movie-script-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-movie-script-final\")\n",
    "\n",
    "print(\"Model and tokenizer saved to: ./gpt2-movie-script-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590877f8",
   "metadata": {},
   "source": [
    "## 9. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb4a9a",
   "metadata": {},
   "source": [
    "### Verify the Fine-tuned Model\n",
    "\n",
    "Check that we're using the fine-tuned model and not the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we're using the fine-tuned model\n",
    "import os\n",
    "\n",
    "# Check 1: Model directory exists\n",
    "model_dir = \"./gpt2-movie-script-final\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"✓ Fine-tuned model directory exists: {model_dir}\")\n",
    "    \n",
    "    # Check what files are in the directory\n",
    "    files = os.listdir(model_dir)\n",
    "    print(f\"  Files in directory: {files}\")\n",
    "else:\n",
    "    print(f\"✗ Model directory not found. Using base GPT-2 model.\")\n",
    "\n",
    "# Check 2: Load and compare model\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load base model for comparison\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Check 3: Compare a few weights to confirm they're different\n",
    "finetuned_weight = finetuned_model.transformer.h[0].attn.c_attn.weight[0, :5]\n",
    "base_weight = base_model.transformer.h[0].attn.c_attn.weight[0, :5]\n",
    "\n",
    "print(\"\\n✓ Model comparison:\")\n",
    "print(f\"  Fine-tuned model first layer weights: {finetuned_weight}\")\n",
    "print(f\"  Base GPT-2 first layer weights: {base_weight}\")\n",
    "print(f\"  Weights are different: {not torch.allclose(finetuned_weight, base_weight)}\")\n",
    "\n",
    "# Check 4: Quick generation test to see if it uses script-like format\n",
    "test_prompt = \"[ACTION] A spaceship flies through space.\\n[CHARACTER:HAN]\\n[DIALOGUE]\"\n",
    "inputs = finetuned_tokenizer(test_prompt, return_tensors='pt')\n",
    "output = finetuned_model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=finetuned_tokenizer.eos_token_id\n",
    ")\n",
    "generated = finetuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n✓ Quick generation test with fine-tuned model:\")\n",
    "print(f\"  Input: {test_prompt}\")\n",
    "print(f\"  Output: {generated}\")\n",
    "\n",
    "# Update the model variable to use fine-tuned model\n",
    "model = finetuned_model\n",
    "tokenizer = finetuned_tokenizer\n",
    "print(\"\\n✓ Model and tokenizer variables updated to use fine-tuned version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate dialogue with context - improved quality settings\n",
    "def generate_dialogue(context, character_name, max_length=200, temperature=0.7, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate dialogue for a character given action context.\n",
    "    The prompt ends with [DIALOGUE] to force the model to generate only dialogue.\n",
    "    \"\"\"\n",
    "    # Build prompt with context + character + dialogue tag\n",
    "    prompt = f\"{context}\\n[CHARACTER:{character_name.upper()}]\\n[DIALOGUE]\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=inputs['input_ids'].shape[1] + 30,  # Generate up to 30 new tokens\n",
    "        min_length=inputs['input_ids'].shape[1] + 8,  # At least 8 new tokens\n",
    "        temperature=0.7,  # Lower temperature for more focused output\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=40,  # More restrictive vocabulary\n",
    "        top_p=0.9,  # More focused sampling\n",
    "        repetition_penalty=1.5,  # Much stronger repetition penalty\n",
    "        no_repeat_ngram_size=3,  # Prevent trigram repetition\n",
    "        length_penalty=0.8,  # Slight preference for shorter responses\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the dialogue part (after [DIALOGUE])\n",
    "    if '[DIALOGUE]' in generated_text:\n",
    "        dialogue = generated_text.split('[DIALOGUE]')[-1].strip()\n",
    "        # Remove any tags that might have leaked through\n",
    "        dialogue = dialogue.split('[')[0].strip()\n",
    "        return dialogue\n",
    "    return generated_text\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Test with Star Wars scenarios - all from Luke's perspective\n",
    "starwars_scenarios = [\n",
    "    \"\"\"[ACTION] Luke Skywalker sits in the co-pilot seat of the Millennium Falcon, gripping the edges of his chair as the ship lurches violently to one side. Another blast from the Imperial Star Destroyer's turbolasers strikes the rear deflector shields. Sparks fly from the control panels in front of him. The warning klaxons blare throughout the cockpit, making his ears ring. Beside him, Chewbacca roars in alarm as more proximity alerts light up the console. Han banks the ship hard to port, and Luke feels his stomach drop as they narrowly avoid another volley of green laser fire streaking past the viewport.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] The detention block is chaos. Stormtroopers lie unconscious on the floor around Luke Skywalker's feet. He stands in his stolen stormtrooper armor, helmet removed, sweat dripping down his face. Han Solo stands beside him at the entrance to cell 2187. The door hisses open and Luke sees a small figure in white robes sitting calmly on the detention bench. Princess Leia Organa looks up at him with a mixture of confusion and irritation. Luke's breath catches - she's even more beautiful than her hologram. She rises slowly, her sharp eyes examining him with obvious skepticism.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] The vast desert of Tatooine stretches endlessly in every direction around Luke Skywalker. The twin suns of Tatoo I and Tatoo II beat down mercilessly on him, the heat oppressive even through his moisture farmer's tunic. He stands alone on a rocky ridge, his tunic flapping in the hot wind that never seems to stop. In his hands he grips a pair of macrobinoculars, pointed at the sky where he saw what looked like a space battle earlier. His aunt and uncle's moisture farm is barely visible in the distance behind him. Luke lowers the binoculars slowly, his young eyes full of longing and dreams of adventure beyond this desert prison he's called home his entire life.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] Luke Skywalker stands in the cargo hold of the Millennium Falcon, facing the ancient Jedi Master Ben Kenobi. A training remote floats nearby, circling him slowly. Luke wears a blast helmet with the visor down, blocking his vision entirely. In his hands he holds an ignited lightsaber that hums with bright blue energy. The elegant weapon that Ben gave him feels both foreign and natural in his grip - lighter than he expected but balanced perfectly. He can hear the old man's footsteps as Ben circles him slowly, his weathered voice offering quiet guidance. This is Luke's first real lesson in the ways of the Force, and his heart pounds with nervous excitement.\"\"\",\n",
    "]\n",
    "\n",
    "starwars_characters = [\"LUKE\", \"LUKE\", \"LUKE\", \"LUKE\"]\n",
    "\n",
    "# Test Luke in non-Star Wars scenarios (out-of-domain)\n",
    "other_scenarios = [\n",
    "    \"\"\"[ACTION] Luke stands at the front of the line in the small coffee shop, packed with the morning rush. The espresso machine hisses and steams loudly behind the counter, making him flinch slightly. Behind him, the line of impatient customers stretches to the door. The person ahead of him just ordered an incredibly complicated drink with seven different modifications, and now the barista is looking at Luke expectantly. He realizes he has no idea what half these drinks even are. His face flushes as he fumbles with unfamiliar paper money in his pocket.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] Luke sits at a cluttered desk in the dimly lit office building, the only person here at midnight. The hum of computers and the distant sound of a janitor's cart echo through the empty corridors around him. Files and photographs are spread out before him on the desk. The fluorescent lights flicker overhead, making his tired eyes ache. He's been reviewing the same evidence for hours, trying to find a crucial clue that keeps eluding him. His coffee has gone cold, untouched.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] Luke grips the subway pole as heavy rain pounds against the windows of the crowded car. The train lurches to a stop between stations, lights flickering ominously. Around him, passengers groan in frustration. Luke checks his watch anxiously - he's already late for an important meeting, and now this. The intercom crackles to life with static as an announcement is about to be made. He closes his eyes and takes a breath, trying to stay calm.\"\"\",\n",
    "    \n",
    "    \"\"\"[ACTION] Luke stands uncomfortably in the grand ballroom filled with elegantly dressed guests. Crystal chandeliers cast dancing light across the polished marble floor. A string quartet plays softly in the corner, music he doesn't recognize. He tugs at his formal wear - expensive clothes that fit perfectly but feel completely wrong on him. Across from him near the champagne fountain stands someone in equally expensive attire, their face twisted in a sneer. The tension between them is palpable. Other guests have noticed and started watching as the other person makes a cutting remark about Luke's background.\"\"\",\n",
    "]\n",
    "\n",
    "other_characters = [\"LUKE\", \"LUKE\", \"LUKE\", \"LUKE\"]\n",
    "\n",
    "print(\"=== STAR WARS SCENARIOS (In-Domain) ===\\n\")\n",
    "for i, (context, character) in enumerate(zip(starwars_scenarios, starwars_characters), 1):\n",
    "    print(f\"Test Case {i}\")\n",
    "    print(f\"Context: {context[:100]}...\")\n",
    "    print(f\"Character: {character}\")\n",
    "    dialogue = generate_dialogue(context, character)\n",
    "    print(f\"Generated Dialogue: {dialogue}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"\\n=== STAR WARS CHARACTERS IN MODERN SETTINGS (Out-of-Domain) ===\\n\")\n",
    "for i, (context, character) in enumerate(zip(other_scenarios, other_characters), 1):\n",
    "    print(f\"Test Case {i}\")\n",
    "    print(f\"Context: {context[:100]}...\")\n",
    "    print(f\"Character: {character}\")\n",
    "    dialogue = generate_dialogue(context, character)\n",
    "    print(f\"Generated Dialogue: {dialogue}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
